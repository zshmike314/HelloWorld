标题与作者
标题: Recent Advances on Federated Learning: A Systematic Survey
作者: Bingyan Liu, Nuoyan Lv, Yuanchun Guo, Yawen Li，来自北京邮电大学。
摘要
背景: 联邦学习作为一种保护隐私的协作学习方法，允许各方在不共享数据的情况下共同训练模型。
目的: 本文旨在系统性回顾联邦学习的最新进展和应用，提出了一个新的分类法，并讨论了当前方法的潜在不足和未来方向。
关键词
分布式AI、联邦学习、神经网络、综述。
引言
DNNs依赖大数据：深度神经网络（DNNs）的性能依赖于大数据，但在现实中，数据往往分散在不同方，且包含敏感信息。
联邦学习解决方案：联邦学习允许多方协作训练DNN，而不需要将数据集中到一个中心服务器上。
预备知识
问题定义：定义了联邦学习中的客户端和服务器端的角色，以及它们如何交互以训练全局模型。
关键挑战：包括异质性问题、隐私泄露和不公平性问题。
联邦学习方法
分类：提出了一个新的分类法，将联邦学习方法分为聚合优化、异质性联邦学习、安全联邦学习和公平联邦学习四类。
聚合优化：探讨了不同的聚合方法，如FedAvg、FedMA和FedProx。
异质性联邦学习：讨论了数据异质性、模型异质性和系统异质性的问题及其解决方案。
安全联邦学习：介绍了针对联邦学习的攻击方法和防御策略。
公平联邦学习：探讨了如何在联邦学习中实现公平性，包括客户选择、模型优化和贡献评估。
流行的联邦学习框架
FedLab：灵活可定制的框架，提供基本功能模块和高度可定制的接口。
Flower：支持在移动和边缘设备上大规模执行FL方法的框架。
FedML：支持多种FL计算范式和配置的库。
FATE：面向生产的平台，提供Private Set Intersection(PSI)和分布式计算框架Eggroll。
FedScale：包含多个现实世界FL数据集和自动化评估平台。
讨论
动态联邦学习：考虑数据在客户端不断变化的情况。
去中心化的联邦学习：不依赖中心服务器的FL。
联邦学习的可扩展性：随着参与者数量增加，如何保持FL的有效性。
统一基准：缺乏统一的基准来比较不同FL方法的性能。
结论
联邦学习的重要性：联邦学习因其在保护隐私的同时生成全局模型的能力而受到越来越多的关注。
综述的贡献：本文提供了一个系统性的综述，分析了FL的流程和挑战，并探索了实际的FL框架。
